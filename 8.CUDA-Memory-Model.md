# CUDA Memory Model

Constant memory and texture memory were born to deal with graphics, in modern GPGPUs are still present and used for data
that must be kept locally to the DRAM (and are read only).

In principle threads from different block cannot cooperate.

## Global memory

Main memory for the device, typically mapped to the device DRAM or shared DRAM in integrated GPUs. It retains data across
several kernel launches. Shared memory inside the GPU on the other hand is not persisten across runs.

Every thread can read/write it. The limiting factor is high access latency, but in principle it's designed for very high
bandwidth (we have thousands of cores and we need to be able to feed them).

For typical static allocation on device, you prepend the qualifier `__device__`. To play with dynamic memory you can use
`cudaMalloc` and `cudaFree`. Lifetime of these variables is typically the entire lifetime of the program.

Similarly global memory uses the `__global__` qualifier. Remember that transfers need to be very regular and well organized
to optimally exploit the memory bus width.

### Memory coalascence

What we refer to as a _coalasced memory access_ is exactly having threads with contiguous IDs that generate accesses to
contiguous memory elements.

```c
A[tid] = ...
```

What happens when you go for offset based and stride based access?

```c
const int c = 3;
float val2 = memA[id + c];
```

With offset you're disaligning your request: you still have contiguous addresses but you're not aligned to the memory
layout. If this happens and you cross the memory alignment boundary you have to pay an additional bus transaction to
bring in the rest of the data. You'll also have useless data.

```c
float val3 = memA[3*id];
```

The strided pattern is worse: it directly reflects in a stride to the values you have when accessing memory.
The worst case is having any stride multiple of eight, already the worst case scenario.

Offset performance shows a kind of on-off behaviour. When having a multiple of the memory alignment boundary as offset
the performance is maximum.

With stride you have a decreasing function with a plateau.

Kernels must always try to maximize coalescence.

## Cache model

In modern GPUs you typically have two cache levels. Clustering of SM is called an island sharing an L2 cache. Mobile
GPUs usually have only a single island and you'll be happy with two levels of cache.

Since hardware managed cache is good for general best effort perforamnce, but to achieve consistent and predictable
performance you can configure memory as scratchpad memory (software configured) or general cache.

Use the directive:

```c
cudaFuncSetCacheConfig(kernel1, cudaFuncCachePreferL1);
```

When we go for something that is not coalesced is because the load-store unit is shared: if all the cores are requesting
data that falls in the same base address, that will be a hit for everyone, otherwise you'll have to deal with this.

The problem with offset based memory access patterns depends on how the data is aligned to memory boundaries. In general
using `cudaMalloc` you're guaranteed the first element is aligned.

With 2D matrices you also need to make sure that the matrix rows are aligned, you can use padding and use
`cudaMallocPitch` function. The pitch is a correction factor and states the padding.

The function returns the pitch for what you want to istantiate, and in fact the parameter is passed as reference.
This value can be used in `cudaMemCpy2D`.

When referencing pitched data in a kernel, ou need to use the pitch instead of the row dimension.

Shared memory is a fast memory within the SM, cannot be accessed by other blocks. Blocks cannot migrate.

Data status is not guaranteed to be preserved between two kernels execution.

We also need to design a shared memory interface. It's organized in 32 banks, with a width of 4 bytes each. You have
several masters writing or reading to a single slave, so it's a single block with at least 32 banks, each with their
own I/O port. This is the only way you can have things happening in the sime cycle.

If your program generates address that lies within a single bank, you need to implement some sort of interleaving policy
to distribute addresses across banks.

It's very common that all (or a big subset) of warp elements access the same element, wihtout any additional measure
you'd need to access 32 times the same data. In hardware there is a broadcast network that relays data to everybody in
the same cycle.

Multicast requests are in form of:

```c
A[id] = A[id % 8];
```

so you are creating four groups and using four banks. This request can still be satisfied in a single cycle, this is
called multicast. As long as N elements are accessing the same address in the same bank, I can relay data.

If one or more threads try to access 2 address of the same bank you cannot solve this problem in hardware, and you
resort to serial access to the bank.

```c
int A[];

A[32 * id] = A[id % 8];
```

in this case I'm still accessing the same bank: bank conflict. NVIDIA engineers were very good at offering the illusion
that this memory has always .

If you design ad hoc pattern to stress the memory you can still make sequential accesses to the same bank.
Since we are programmers that want to achieve the maximum performance possible, you need to avoid these patterns.

This is different from coalescence: coalescence refers to the path from L2 to main global memory (DRAM), because you need
to create requests that _efficiently use all the wires_.

Messy patterns that still access to different banks are as performant as "neat" ones. Requests are serialized, so if you
have N requests, it will take N cycles to respond.

## Usage of shared memory

Dynamic allocation is used when you don't know in advance how much shared memory you need.

Static allocation:

```c
__shared__ type identifier[MEMSZ];
```

while dynamic allocation:

```c
extern __shared__ type *identifier;
```

**You cannot allocate multiple dynamic arrays** (the triple bracket notation does not allow it) so the idea is that you
pass only a single vector and you manually divide it using pointers or by allocating a structure.

For example column access in matrix multiplication generates a unamenable stride that worsen the performance. But we can
do better. Since MMUL access a ot of times the same addresses, we can exploit shared memory (broadcast) that is only 48KB
in total and reach good speedups.

There's no way we can accomodate a structure of a relevant program in 48KB (16KB is always reserved for cache).

Loop tiling is a transformation that tries to exploit the locality in programs. In compilers was to improve the cache
access, but here it's fundamental because you need to optimize the access of shared memory.

CUDA mandates the manual tiling of matrices every time you need decent performance. It's matter of keeping the tile
dimension to mantain tiles in memory as much as possible.

We could use cache, but they are totally reactive and they cannot foresee that some data could have been useful alter on.
They continuously evict data.

The difference is having a full predictable behaviour.

## Synchronization

As usual, synchronization can be done within a single block (to sync between blocks you need to use global memory, but
it's very inefficient because you're forcing some blocks to depend on others and it's also risky because of potential
deadlocks).

```c
__syncthreads();
```

You need to make sure each thread will eventually encounter the `__syncthreads`, otherwise you will incur in a deadlock
and see weird side effects of dead locks.

## Virtual memory

Well established mechanism that makes possible to run different processes on the same machine, and have them believe
they have the whole memory available to themselves.

The CPU has an MMU, designed as part of the load store unit of the CPU, or the L1 cache hierarchy level. The virtual
page number associated with the process id provides the physical page address.

There are different tools to exploit the MMU. But what the problem is about?

A CUDA program is originated by the CPU, if it uses dynamic memory (via a `malloc` call) is going to reserve space from
a heap. The problem arises when you want to transfer data to a GPU but you have two problems: physical mapping and paging.

if the matrix is bigger than a page size, you'll need to span several pages. When you're crossing the boundary of a page
you might end up in a physical page that it's not contiguous to the previous.

### Memory spaces

You have a regular paged memory and another space which is not paged and not cacheable but there you will implement the
buffers for communication. Performance-wise this implies an additional overhead: you have an additional copy
to a contiguous non-paged area that will provide the base for the `cudaMemcpy` under the hood.

There's no swapping here so there isn't this overhead and data can be sent directly.
You tend to have higher performance, you can exploit the contiguity of data. You don't want this portion of the DRAM too
high.

### Pinned memory

Allocated directly on the non-pageable area.

## Pointers

C and C++ makes dynamic structures really easy with the use of pointers and node-pointers chains. A pointer carries
information about the physical address in memory, that has no meaning on the GPU addressing space: you need to translate
this pointer, but that brings up the problem of having to copy the data pointed by the address that you're translating.

Let's say you have a struct with a pointer to a string (char *), you will also need to reserve a buffer for the string.
To process those kinds of structures you need deep copies.

If we could somehow share (in a physical of logical manner) the same pointers that are valid for the CPU and the GPU, we
would entirely get rid of the problem and the need of deep copying.

An IOMMU can be instructed to rely on the same page translation tables used by the CPU. You have the simpler model again,
but you will need to copy it. You can design the system to silently copy data when needed.

You now have a virtual memory system on top of a distributed memory system. Not as efficient performance-wise: it's reactive,
same behaviour as caches (or lazy).

`__shared__` memory makes the system proactive, and it allows to write your program and remove additional cudaMemcpys: it's
all a shared address space.

This has the major benefit of simplifying porting of programs to the GPU. One thing is enabling the execution of pointer-based
code on the GPU, another thing is making it efficient.

### Caveats

We are not prefetching, and we are using small chunks of data not making the best use of the memory channel bandwidth.

## Integrated GPUs

Things tend to change a lot: you have a single physical memory and you don't need to move data across the memory channel.
You only need to move data across the same DRAM. If you implement UVM on top of an integrated GPU (shared memory architecture)
you tend to have performance very similar to pinned approaches.

Discrete GPUs make better use of pinned memory, since you manually control memory transfers and don't have to directly
access the DRAM (although possible). UVM on PCIe is slow since you have to access DRAM, it only makes sense when accessing
small chunks of data once.
