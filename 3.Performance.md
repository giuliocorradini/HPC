# Performance

What things should we care about when modelling for performance? The three main factors:

- coverage, the amount of parallelism you can find or exploit in the algorithm;

- granularity, partitioning among processors. Balance between overhead and parallelization.

- locality, computation and communication. Making sure your loads and stores are not too slow.

## Coverage

Not all programs are emabarassingly parallel, so not all programs can be executed in parallel.
Programs have an inherent sequential part, with data dependence, then there is part of the program with parallelism.

Loops: each instruction is independent of the previous.

Can we get a linear speedup? I'd like an 80 speedup with 100 processors. What fraction of the program can be sequential?

The problem has been formalized by Amdahl's Law. The law states that the performance improvement that cna be gained out
of a faster mode of execution (use of parallelism, accelerator, or anything that allows the program to run faster) is
limited by the fraction of time you can stick to that faster mode of execution.

The formal law is:

$$
\text{speedup} = \frac{1}{(1-p) + {p \over n}}
$$

where (1-p) is the fraction of time for sequential work, whereas p over n is the fraction of time spent in parallel
computation.

With an infinite number of processor I asimptotically get to 0 in parallel part, but you're always left with sequential
part.

The law explains why after the Pentium 4 performance gains was of 22% each year. Instruction level parallelism is limited
in program and a similar observation can be made here: programs have certain operations that cannot be executed in parallel.

This is why multicores have kept a small number of cores. If the target is parallelizing legacy code, or legacy code
executed in parallel there's never the case embarassing parallel code.

That's why GPUs need embarassingly parallel applications, and graphics is the case.

The law is a purely ideal **upper bound**, but in real cases the speedup is less than ideal. That are cases however where
you get superlinear speedups out of parallelization.

### Superlinear

Cache miss rates impacts code, that determines a drastic reduction in the linear latency. Superlinear speedup are possible
everytime I make better exploitation of locality which translates in smaller number of misses.

### Overhead

This is the biggest barrier in getting the desired speedup. The overheads include the cost of starting a thread or a process,
communicating shared data, synchronizing and extra redundant computation.

This is where the issue of granularity comes into picture: there's a tradeoff between getting good performance and reducing
overheads.

## Granularity

Recall fine grain and coarse grain parallelism.

Coarse grain has higher useful to additional work ratio since you pay the overhead of creating threads once, whilst fine-grain
parallelism have more frequent sources of overhead.

Load balancing is making sure every thread gets the same amount of work. Unless the computation is already evenly balance
by design, a coarse-grain parallelization scheme is not the best option.

Slowest core dicates the parallel execution time. In static load balancing you try to figure out ahead of time the best
partitioning and code in the program the amount of work for each thread: best efficient for homogeneous multicore and not
very efficient for heterogeneous multicores (uneven distribution of cores).

Dynamic load balancing has the property that the system is reactive to idling: when a task is idle, it queries the system
for additional work to do.

Finer granularity means more opportunities to balance the load, but every time you context switch there's is a potential
overhead.

## Communication

It's quantified with latency (the amount of time taken to complete an operation) and bandwidth (it's not about absolute
timing but throughput: the amount of information we can communicate in a period of time).

Using an interconnect: for every cycle, the interconnect is able to handle a single transaction usually. This is latency
while bandwidth is about how many bytes I'm allowing in each transaction.

The other is Network-on-Chip: this is the example we used for non-uniform memory access (NUMA) systems. These systems
use one cycle per hop, and that dependes on the physical layout of the memory.

Caching is about hiding latency.

$$
C = f \cdot (o + l + \frac{n \over m}{B} + t - \text{overlap})
$$

once you have established the channel, which involves some overhead, the more bytes are transferred the more amortized
the channel creation becomes.

If multiple processors are trying to take owenership of the bus at the same time, they'll involve in a contention that
takes up some time.

Overlapping computation with comunication is the key to make this visible cost C small.

As programmers the most typical way of reducing cost of communication is coming up with some sort of double buffer
communication: assuming the system has a DMA which transfers data for the CPU. This can be achieved with pipelining.

## Locality
