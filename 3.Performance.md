# Performance

What things should we care about when modelling for performance? The three main factors:

- coverage, the amount of parallelism you can find or exploit in the algorithm;

- granularity, partitioning among processors. Balance between overhead and parallelization.

- locality, computation and communication. Making sure your loads and stores are not too slow.

## Coverage

Not all programs are emabarassingly parallel, so not all programs can be executed in parallel.
Programs have an inherent sequential part, with data dependence, then there is part of the program with parallelism.

Loops: each instruction is independent of the previous.

Can we get a linear speedup? I'd like an 80 speedup with 100 processors. What fraction of the program can be sequential?

The problem has been formalized by Amdahl's Law. The law states that the performance improvement that cna be gained out
of a faster mode of execution (use of parallelism, accelerator, or anything that allows the program to run faster) is
limited by the fraction of time you can stick to that faster mode of execution.

The formal law is:

$$
\text{speedup} = \frac{1}{(1-p) + {p \over n}}
$$

where (1-p) is the fraction of time for sequential work, whereas p over n is the fraction of time spent in parallel
computation.

With an infinite number of processor I asimptotically get to 0 in parallel part, but you're always left with sequential
part.

The law explains why after the Pentium 4 performance gains was of 22% each year. Instruction level parallelism is limited
in program and a similar observation can be made here: programs have certain operations that cannot be executed in parallel.

This is why multicores have kept a small number of cores. If the target is parallelizing legacy code, or legacy code
executed in parallel there's never the case embarassing parallel code.

That's why GPUs need embarassingly parallel applications, and graphics is the case.

The law is a purely ideal **upper bound**, but in real cases the speedup is less than ideal. That are cases however where
you get superlinear speedups out of parallelization.

### Superlinear

Cache miss rates impacts code, that determines a drastic reduction in the linear latency. Superlinear speedup are possible
everytime I make better exploitation of locality which translates in smaller number of misses.

### Overhead

This is the biggest barrier in getting the desired speedup. The overheads include the cost of starting a thread or a process,
communicating shared data, synchronizing and extra redundant computation.

This is where the issue of granularity comes into picture: there's a tradeoff between getting good performance and reducing
overheads.

## Granularity

Recall fine grain and coarse grain parallelism.

Coarse grain has higher useful to additional work ratio since you pay the overhead of creating threads once, whilst fine-grain
parallelism have more frequent sources of overhead.

Load balancing is making sure every thread gets the same amount of work. Unless the computation is already evenly balance
by design, a coarse-grain parallelization scheme is not the best option.

Slowest core dicates the parallel execution time. In static load balancing you try to figure out ahead of time the best
partitioning and code in the program the amount of work for each thread: best efficient for homogeneous multicore and not
very efficient for heterogeneous multicores (uneven distribution of cores).

Dynamic load balancing has the property that the system is reactive to idling: when a task is idle, it queries the system
for additional work to do.

Finer granularity means more opportunities to balance the load, but every time you context switch there's is a potential
overhead.

## Communication

It's quantified with latency (the amount of time taken to complete an operation) and bandwidth (it's not about absolute
timing but throughput: the amount of information we can communicate in a period of time).

Using an interconnect: for every cycle, the interconnect is able to handle a single transaction usually. This is latency
while bandwidth is about how many bytes I'm allowing in each transaction.

The other is Network-on-Chip: this is the example we used for non-uniform memory access (NUMA) systems. These systems
use one cycle per hop, and that dependes on the physical layout of the memory.

Caching is about hiding latency.

$$
C = f \cdot (o + l + \frac{n \over m}{B} + t - \text{overlap})
$$

once you have established the channel, which involves some overhead, the more bytes are transferred the more amortized
the channel creation becomes.

If multiple processors are trying to take owenership of the bus at the same time, they'll involve in a contention that
takes up some time.

Overlapping computation with comunication is the key to make this visible cost C small.

As programmers the most typical way of reducing cost of communication is coming up with some sort of double buffer
communication: assuming the system has a DMA which transfers data for the CPU. This can be achieved with pipelining.

## Locality

There have been many model for distributed memory access. In DSM (sometimes also referred to as PGAS, partitioned global
addressing space).
UPC was proposed for DSM, contains instruction to bind the execution of instruction to some processor. This is something
that always gets reflected on the programming model.

The metrics for locality we want to optimize are: memory latency (reduce) and memory bandwidth (maximize). This introduces
some overhead: make sure it's well worth introducing additional overhead for optimizations.

Improving the locality is mostly about reusing data, where do we get locality: if computation exhibit data locality, we can
either play with displacement (matrix layout and access behaviour) or reorder traversal.

For example, at distinct iterations I and I', A[j] is reused:

```c++
for (i=1; i<N; i++)
    for (j=1; j<N; j++)
        A[j] = A[j+1] - A[j-1]
```

this is a good target for a loop interchange (change the order of the traversal).

For matrices-like structures, access happens in rows order.

Based on the reuse in your code, you can apply reordering to improve locality. To this aim a **loop permutation** is
the simplest transformation that makes data faster to access. Just chaning the loops, you change the way data is accessed
and this improves performance.

Another transformation in loop tiling or blocking, particularly targeted at structures that can be processed in tiles.
You choose the size of blocks to maximize the retention of data in the cache, and make the most use out of it.
A common exploitation of tiling is done in matrix multiplication.

When we mentioned superlinear speedup, even in single core-context, this will have a big impact. This is an additional
benefit coming from better exploitation of the caches, and not additional power coming from multiple processors.

### Tiling

Very important, and on GPUs is fundamental for performance. Tiling is very commonly used to manage limited storage and
can be applied in registers, caches, software-managed buffers (offered by GPUs), small main memory.

**Can be applied hierarchically**

To exploit parallelism at different level, and overcome for example the bottleneck for a single shared memory. That's the
reason why today's memories are designed as individual banks: the idea is that on average if you lay out data uniformally
across the banks, it will statistically be difficult that all the processors will to access the same data at the same
time.

Continguous elements of the array are allocated in different memory banks. The ideal would be that different threads would
access different banks, but threads are assigned contiguous elements, that are mapped in the different banks and you will
create a bottleneck. All threads will try to access the same bank..

Solution: reshape the data layout to exploit parallelism at memory system level.
