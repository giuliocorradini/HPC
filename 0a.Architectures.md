# Architectures

Modern systems are composed of different computing devices (CPUs, GPUs) often all in the same die, and you must be
able to write code that can run on different chips.

> Anywhere at the chip level we have parallelism and heterogenuity.

Furthermore the CPU complex is most of the times locally homoegenous (although architectures such as arm big.LITTLE do exists).
Take NVIDIA Xavier SiC as an example, we also have a Volta GPU.

CPU have a cache hierarchy, mostly transparent to the programmer that does not have to deal with them. This is not the case for
accelerators such as the GPU, though to some extent they use caching mechanisms. The memory, for performance reasons, is
manually dealt with. The caching mechanism is not automatic like CPU.

The cache of GPU is a SRAM buffer where you can move explicitly memory.

## How did we get there?

The main result is related to performance. From circa 1985 to 2003 the performance of CPUs have been constantly increasing
(following Moore's Law). The benchmark remained pretty much the same. The simplest way to improve performance was to increase
CPU clock frequency and make the CPU run faster.

Other strategies bear to improve the performance further: pipelining above all. The most important thing is that CPU
design principles remain pretty much the same.

Nowadays commercial processors run up to 4/5 GHz, but there have been faster processors (hobbyist projects or enterprise stuff).
The performance increase isn't as good as it used to be, and processor perofrmance grows only 22% a year.

Moore's Law still holds, but where getting to the size of the atom where it's impossible to further shrink transistors size.

### Intel roadmap for CPU

Report from Intel with a forecast of CPU performance. Their forecast was that by 2013 they would get to 25GHz clock rate, but
this never happened. Two years after they corrected their roadmap and understood that clock speed wouldn't get any higher.

They started a new path since, that's going down in frequency.

### It's all about physics

Dennard states that

> transistors can be scaled down by 0.7x every technology generation.

As transistor size is reduced, the electrical signal will have a shorter critical path to traverse, thus frequency can be
increased.

That means that if circuit delays is reduced by 0.7x, operating frequency increases by 1.4x (1/0.7x).

$$
\text{power} = \text{capacitive load} \times \text{voltage}^2 \times \text{frequency}
$$

The problem therefore is power density: I'm trying to squeeze the same power in a smaller chip that is more difficult
to dissipate.

Dennard simply states that although I can increase frequency, the power must remain the same.

A good news: as we scale the technology down, the capacitive load goes down. That constant is inherent of the manufacturing
process.

Power dissipated by a circuit is composed of two parts:

$$
\text{power} = \text{power}_{dyn} + \text{power}_{static}
$$

The part of $\text{power}_{dyn}$ is power burnt when the CPU is running. But circuits consume memory even by powering them
and that part is $\text{power}_{static}$.

$$
\text{power}_{static} = k_d \times \text{voltage} \times \text{current}_{leak}
$$

Ultimately more heat translates to unreliable processors. Cooling of CPU was a big thing back in the day (it still is
but less than before).

## Pipelining

In a 5 stage pipeline, where we get to a steady state I'm executing five instructions per clock cycle. This is the simplest
form instruction pipelining.

Some processors have replicated pipeline stages and support VLIW (very long instruction words), you can schedule two
instructions at a time.

Superscalar or out-of-order execution is more sophisticated. The limit of pipelining is given by data dependencies and
data hazards: two instructions where the execution of the second instruction depends on a previous instruction wirting
something on the register file.

Back in 90s/early 2000s they really believed in deep pipelining. Instruction can be reordered to mantained the pipeline
in a steady state as long as possible. As a programmer I don't want to deal with it, so I want a pipeline that allows
any instruction in any order and if I get to a data hazard I'll stall the instruction up until their dependencies are
met. With replicated pipeline stages, I can stall single instructions, but that makes the CPU project more complex.

The commit unit is a part of the datapath that assures the data stays coherent to the execution order.

Out-of-order processors are still a thing.

The amount of ILP present in a program is the limit of ILP. What's the ILP there is in a program? Can we have a 100%
independent instructions program? No, otherwise it would all be dead code. I'm doing something useful on data, that
requires dependent instructions.

## Walls

The power wall is not the only.

### The ILP wall

Processor can reorder instruction in the pipeline and do aggressive branch prediction, but this increases esecution
unit complexity and associated power consumption.

We got to a point where power consumption was unbearable. Serial performance acceleration given by ILP stalled.

### The memory wall

Accessing data is a limiting factor. There is a processor-memory gap of 50%. DRAM uses a completely different technology
and is very slow. It's better to spend transistors to improve memory behaviour.

In an ideal computation model, each load/store costs only a single operation. The main way engineers solve this problem
is by designing deeper and deeper cache hirearchies.

## Multicore

What happens in single core if I increase the frequency of 20%? Performance increases by 13%, but power goes up by a 73%
all relative to the baseline.

The idea of clocking start getting the other way (based on this assumption). If I underclock by a 20%, the power consumption
gets halved. That's perfect for the power budget: now duplicate the core, and spend pretty much the same power but performance
goes up by 1.73x.

New idea: shorter pipelines, replicate CPUs, reduce frequency slightly. If I consider a traditional design I'd have a large
core with cache: the transistor budget is all spent in a complex pipeling and a deep cache.

If I implement the same ISA in a simpler design (simpler branch predictor, less pipeline stages) I get half the performance
for just a quarter of the power budget. I can replicate this design by four and get double the performance for the same
power budget as the large core.

Multi-core seemed power efficient and better at power and thermal management.

Running the benchmark unmodified, thus on a single cpu, halves the performance (since the simpler single core is half
as performant as the old larger one).

Now sequential app performance stalls and typical power stalls (for the considerations made earlier where we reduce
complexity of single cores). Each generation you have the same number of transistor: use it wisely, or half the size of
the die.

Performance of parallel application starts to grow steady with transistor density again. But this is all peak performance.

Peak performance is achieved only by operating each core at full throttle: this course focuses entirely on bridging the gap
between sequential and parallel performance.

## The problem of dark silicon

> Unused area of the die is growing exponentially with increase of cores. This is called dark silicon.

Each generation, cores increase by a factor of 2 and frequency stays the same. But this is not the case the case because
of power density.

That comes from the fact that leaving the frequency unvaried, if we implement four times the cores you may not be able to
switch them all and that becomes dark silicon.

The fourth wall is **the utilization wall**, and it's the problem of not being able to utilize all the area.

In two generations I have two options: downclock the cores and increase them, or increase dark silicon portion and keep
what is left at full speed. These are all tradeoff.

Four solutions:

- the shrinking horsemen: remove dark silicon. But you use dark areas also, rarely, for example caches when executing
small working sets apps. The idea is rather than avoiding using that area, you heterogeinise and solve the problem altogether;

- the dim horseman: utilise transistor as a lower frequency. Dimming can be achieved by spatial dimming or temporal dimming.
The former involves switching transistors at a lower transistors, the latter assumes that the system is thermally limited
and so you implement smaller cores for everyday use and a bigger core for heavy lifting operations. Perfect for battery
operated devices. It's convenient since big and LITTLE share the same ISA. It's over time that I dim the silicon;

- the specialized horseman: most relevant to what we're discussing here. We'll use all of the dark silicon to build
specialised sections to run code. This is the approach of accelerators, for example video decoder or encoders.
While I'm executing video decoding the CPU is basically off, when moving work to an accelerator I'm consuming way less
energy;

- the deus ex machina: the fundamental problem is MOSFETs, we can go beyond CMOS technology.

## Advantages

The advantage of multicore is the effective use of billion of transistors (reducing the problem of dark silicon).

Potentially it's very easy to scale, just keep adding cores for higher performance. Although each processor can be less
powerfull, it's cheaper to implement and run many replicas and distribute.

The main disadvantage is that parallelization is not a limitless way to infinite scale performance. Processors are sharing
memory and buses, and this cause contentions where two agents needs to access the same resources.

Furthermore programs can only be parallelized to some extent, but some part of them is inherently sequential and execution
cannot be split on multiple cores.

The adoption of a parallel programming model wasn't straightforward since senior developers were already accustomed to
a sequential programming model. Programmers were put to challenge in sharing the task to multiple processors and to always
keep them busy.

The best way to achieve high performance out of parallel systems (with many cores) is to learn to write parallel programs
but in the beginning this was not the way: back then developers tried to find ways to **transform** programs.

A lot of emphasis was put on compiler optimizations, and deal with parallelization automatically.
