# CUDA

It's clear Nvidia had in mind to provide a single programming model across their devices. You have a common instruction
set, with specific capabilities of each device.

CUDA is based on LLVM, and the backend is proprietary.

On top of that you have a set of developer tools, such as debuggers and profiles. A software layer gives access to the
GPU virtual instruction set: you write compute kernels with this.

For tuning developers, be aware of PTX which is the name of that virtual instruction set.

There are some libraries for CUDA to perform the most common operations in linear algebra, randomization etc. cuBLAS,
cuFFT, cuRAND, cuDNN for deep neural networks, nvGRAPH for graph analytics which are not data parallel.

If you want something like page rank, where you have to traverse heavily the graph, you have to completely change the
algorithm and use a matrix representation of the graph.

The grid is the number and organization of involved threads.

The serial parts run on the CPU, and kernels are offloaded to the GPU.

### Execution

You have the regular function identifier, the name of the kernel, and you justappose three angular brackets to launch
the kernel.

```c
kernel<<<num_block, num_thread>>>(params...)
```

## Porting

`__global__` is a qualifier for kernels that run on both hosts and targets. Can only be called by the host.

`cudaMalloc` works similarly to the standard malloc. Memory is allocated on the device DRAM. Returns the address to the
host but the memory is actually on the device.

`cudaMemcpy` works similarly to memcpy, but you can specify the target and direction (host x device).

## Unit of work

You tipically have lots more of threads than scalar processors, lots more of thread blocks for each multiprocessor.
On NVIDIA GPUs there's no notion of binding thread blocks to processor, opposed to NUMA CPU where you want to bind thread
to avoid moving memory.

If you have dependencies in your code, you have to explictly encode them. Once a block is assigned to a SM does not migrate
to other SM. If you migrate, you would need to migrate the context and there's no hardware support for that.

SIMD units are 32 cores wide and they're called warps: they're more of the unit of work mapping on the SIMD hardware. The
scheduler selects a warp for execution. Every CUDA core executes on the 32 threads in the warp.

## Registers

In NVIDIA GPUs, registers are dynamically partitioned based on grid configuration. You don't have a prefixed number of
registers per block, and it's the moment you realize if the number of blocks requested is feasible.

Let's say you have a GPU with a streaming multiprocessor architecture 