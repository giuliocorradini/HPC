# Parallel programming in practice

What do you have to do as a programmer to implement parallelism or write a parallel program from scratch?

First of all you have to identift what work can be parallelised. Where are the hotsposts? I.e. the part of the program
that can be parallelised.

1. Identify the hotsposts and make sure it's worth the effort.

2. Partition work (and also associated data). For example partition data structures.

3. Managing data access: explicit or implicit communication (depending on distributed or shared memory system) and of
course synchronization. Do I have race conditions?

Parallelization goes through decomposition, assignment, orchestration and mapping.

> A parallel task is part of the program that you can assign to one of your threads.

Assignment is next in line: how do I assign tasks to my threads? You want each thread to have a similar amount of work
to do, otherwise load will be unbalanced.
Each worker could be a node, a thread etc.

Orchestration means that if your task have any type of dependency (e.g. task J won't start until task I completes), or
where to put messaging instructions, and synchronization.

Many processors can schedule more than a single thread on the same CPU, and that refers to the capability of a CPU to
context switch between two threads quickly. We can time multiplex the CPU over several abstract CPUs, multiplexing over
several processes is slower and techniques such as multithreading improve the situation. Hardware-assisted threading.
GPUs do rapid context switching assisted by hardware threading support.

Binding a thread to a specific core is a typical problem you face related to mapping. Working with the same data on cores
will improve the performance by a lot, rather than fruitlessly moving around memory.

## Decomposition

The idea is to create at least as many tasks as you have execution cores. If you have less tasks than cores you can't exploit
parallelism. Usually you create many more tasks than available processors.

The challenge here is to identify hotspots, that are limited by data dependency.

A very good example is an MPEG encoder, there are several stages starting from VLD and you can take two paths. You have a
reference frame, where you extract relevant features and subsequent frames encodes the movement of reference features.
The encoding is the computation of these movements.

When you have a sudden change of subject, a new reference frame is inserted. You could opt for a coarse grained task
decomposition, so out of the two paths following VLD you identify two tasks.

Further refining the parallelism for coarse grained tasks you can implement a pipeline there, and the steady state is
achieved when each thread operates on data coming for the previous.

Then you have motion compensation, data is much more parallel, so you have something much more like SPMD.

### Guidelines

Don't have any, it's mostly about experience. It's about your skills spotting decomposition opportunities.

- Function calls are a very easy way to start the search: they naturally tend to get outlined as coarse grained tasks;

- loop iteration are you main source of data level parallelism. Be aware of data dependencies;

- always start with the finer grained decomposition as it's easier.

You may also make sure that the program is _flexible_, not tied to a particular platform, but it depends on what's your
job. In general you should opt for something that can be generic and easily changed in terms of parameters.

*Efficiency* should be regarded because of load balance: you make sure decomposition creates suffficient work to ammortize
the overhead of parallelism.

*Simplicity* is the key of software engineering: better to have sequential code than parallel code which is unreadable and
more difficult to maintain.

### Data structures

Which categories can be decomposed? You usually work with arrays, matrices and in general if you don't deal with very
sparse matrices otherwise programs tend to encode matrices in smaller representation.

The representation is not regular anymore. They're more like linked lists.

Recursive data structures, the other type you will have to deal with, the decomposition is usually done in chunks: subtrees
out of trees or exploit parallelism out of recursing patterns.

## Dependency analysis

Bernstein's Condition => two task can be safely run in parallel.

Two tasks are parallel if respective inputs are not the respective outputs, and outputs do not overlap.

## Gauss-Seidel acceleration

We have a stencil, for each data point you depend on the surrounding values. How the computation happens cross elements?
Each row depends on the previous and on previous elements on the same line: there is independency on the diagonals.
This is called _waveform type of parallelization_. Typical way of exploiting parallelism.

Problem: border effect where you don't have sufficient parallelism, and each diagonal create a dependency with the next
one and you have to synchronize. Fork-join model, and you tend to synchronize quite a lot.

You may change the order cells are updated, so you change the algorithm itself to better exploit parallelism. An approach
can be red-black coloring. This works because the solver is iterative, and the new partial solution will be different
but you will eventually converge to the same proper solution.

Alternating coloring: then you update reds first and then blacks. This is parallel **by design**.

**I'm changing the algorithm** and this is done quite often, for example on GPUs, for the sake of exploiting parallelism.
Stuff that on CPUs works perfectly and GPUs don't, since they're optimized to work on big array-like structures and they
don't like many pointers.

We're exploiting the inaccuracy of the algorithm. The solution was not perfect by definition, but I will get to a point
where the error is so small I can consider the solution equal. Approximate computing: there's a lot of approximation and
we don't care for perfect solutions. And by definition, of floating point, we are approximating something that cannot
be represented with infinite precision.

## Assignment

Assigning tasks to a thread is the process of distributing the data to be processed across workers. We want to achieve
goog balance and small communication cost. You can statically do this (at build time), a typical example is when you
unroll loops and encode for each threads the iteration they will pick.

A dynamic approach is based on the fact I don't know what thread will pick which data. Threads asks the system for more
work to be done.

The key problem here is load balancing: in this case and one of the four takes twice the amount of work. The longest
thread sets a lower bound for execution time across all workers.

Grid solver: two opposite approachers. You have blocked assignment and interleaved assignment, does it matter for the
sake of load balancing?
Yes, the overhead of fine grained is more. Cache behaviour is important.

In asymmetric systems it's better to use an interleaved assignment of work (NUMA). Making smaller units of work is not
something good per se for load balance, but then you need to avoid static assignment and go for something completely
dynamic.
