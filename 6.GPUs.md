# On GPU programming

Dedicated hardware such as ASICs completely lacks flexibility and have high costs, that only big players can afford.

GPUs tend to sit between fully hardware implementations and CPUs. They're 3 to 4 orders of magnitude more efficient than
CPUs.

In between we have FPGAs, which is full hardware implementation of functionality, but it's not really printed. It's
reconfiguration of this hardware that provides flexibility.

GPUs bear for graphics. Overtime people realize that this type of parallelism is abundant in linear lagebra, deep learning
etc.
GPUs designers realised that by designing their GPUs in a slightly different manner, they could take the market of high
performance numerical computation that was looking more and more into accelerated solutions.

The original task of GPUs was to compute polygons, texture mapping and shading. Therefore the cores are called Shader Cores,
then a number of other units doing very specific activities.

In a typical CPU you have the usual pipeline stages, with fetch/decode unit, ALU, register file and data cache to hide
the big latency of memory.
On top of that we may want to add branch prediction to avoid starving the pipeline, pre-fetching...

### Paradigm shift

Since I'm dealing with very specific type of operation, that heavily relies on data parallelism, maybe I don't need the
hardware for components that helps a single instruction stream run faster.

We can replicate several times this very simple unit to perform computations. Now I have lots of ALUs, F/D units and
execution contexts.

But looking even more carefully, as I'm most concerned about data parallelism. Why do I need to care about multiple F/D
units? The only benefit is to drive different ALUs with different instructions but that's not our case.

> Let's have a single fetch/decode unit that drives multiple ALUs, and have a shared context data. That's why GPUs are
not so good on general purpose computations, since you have a single fetch/decode unit that imposes sequential execution.

Now we have something more similar to a SIMD unit. We don't refer to GPUs as several cores, but only a bunch of cores that
have very large SIMD units.

Let's say you have an NVIDIA GPU with 16 SM, each having 128 CUDA cores. The CUDA cores are the ALUs and provide the SIMD
width.

You're looking at embarassingly parallel type of for-loops.

Since my fetch/decode unit is pretty fat and I mostly do embarassingly parallel fors with the same operation on multiple
data, why don't we share the F/D unit?

### Branches and hiding memory latency

SIMD does not imply SIMD instructions.

What happens with branches? Sequential execution of both branches with each thread taking the first branch then the second.
The solution is to limit conditionals across streaming multiprocessor, as each streaming multiprocessor share the same
lock and they poll and execute the same instruction.

Control flow divergence, so-called because in the control flow you have divergent graph, is what you want to avoid.
It's not the only form on stalls you may encounter.

In general you have a stall because of dependencies. Mostly in GPUs you don't get fancy caches (nowadays not mostly true
but you still want to manage memeory manually).

To hide stalls you need to context switch a streaming multiprocessor, when waiting for some memory. In software you have
several blocks, that can be scheduled on a streaming multiprocessor to hide stalls. You want hardware to have plenty of
threads and plenty of blocks to play with.

The active state of any thread is the register file, the instruction pointer and stack pointer. To avoid costly context
switches, GPUs replicate the register file (plus whatever is need to point to your current stack, heap etc.). By having
the hardware to quickly switch, you don't need to store that status in memory.

## Key ideas for GPUs

Have many cores, slimmed down, to run in parallel.

Pack core full of ALUs and share instruction streams across group of segments. Expliit SIMD instruction, implicit sharing
managed by hardware.

Lastly avoid latency stalls by replicating register files, and do fast context switch.

## Moving data

In heterogeneous parallel programs, there is a lot of data movement.
CPU-GPU system with discrete GPU with L3 shared cache across CPU cores. What goes across the memory channel, has smaller
bandwidth and higher latency.

Moving on device side, to keep all of the streaming multiprocessor and the CUDA cores in them, you need a lot more of
memory bandwidth.

On-chip data can be of different type: texture caches that are read only data, and local memory that is writable.
