# Coherence

There are novel issues regarding memory: coherence. This is introduced by the use of shared memory.
Each processor have a copy of data in their memory. If someone modifies this data, it will become incoherent.

Consistency is a similar problem, related to possibly changing the semantics of my program because of incosistent
memory access given by non deterministic execution on CPUs. This is very strict and case pose a serious limit to
performance of these systems.

Synchronization refers to the fact that multiple threads can both read and write values: the order in which these actions
are performed have a relevance.

## Single CPU

In single CPU systems, coherence memory arise from the use of the cache. As soon as the CPU updates the value of A, there
is a coherence problem since the cache contains a value that's not the same in the memory. If an I/O device tries to read
the same value of A, there would be a problem since memory in not coherent.

## Multi-processors

The main problem is that caches, although necessary for perfomance, create copies therefore incoherency. Getting rid of
the caches is not feasible. As long as caches aren't writing memory, there's no problem.

## Keeping cache coherent

It's architect's joib to keep cache values coherent. When any processors ahve a miss cache, notify other processors.
The first approach to this problem is the **snoopy cache**.

The cache snoops transactions on the bus and checks that there are no writes to location that already missed.
The protocol works like:

- upon a write miss, the address is invalidated in other caches before a write is performed;

- upon a read miss, is a dirty copy is found in some cache, a writeback is performed before the memory is read.

### MSI protocol

The address of the cache line are divided in: index of the address, offset. The offset specifies the size of the line.
Memory is never accessed with a single machine word, instead it's accessed by lines which are multiple of machine word.

In a direct mapped cache, each address having the same offset bits, will make its way in the same line.

We need to add a few bits to distinguish between states: modified, shared, invalid.

These states are handled with a finite state machine. Starting from S, on a read miss (P1 gets line from memory), if any
processor read, this state is not modified.

If another processor tries to write the same address (single line), to reflect the change I should invalidate locally
my copy, so I go to the invalid state.

On the contrary, if it's me trying to write on the value, I'll go in the modified state. Now what happens if P1 reads
or write the value? Nothing, I stay on the modified state.

What happens if someone else writes? I go to the invalid state. Once there, If a processor tries to write on a line that's
invlaid for me. For the other processors to see the most recent data, It depends on the eager/lazy approach of processors.

When in M state, and a processor tries to read the value, I write it back to DRAM and go to the S state. If my first
action is a write, I enter the modified state.

Incoherence arises as soon as another processor tries to read or write data.

### Performance

The main caveat is perfomance: any operation generate additional memory requests that are not part of the problem, to
keep the memory consistent. Additional traffic on the bus.

Is it really necessary to treat all data as shared? No, this can be further specialised.

Parallel programs use only some shared data:

```c++
int s = 0;
for (int i=0; i<N; i++)
    s += A[i];
```

S is shared, i isn't and it would be a mistake to share the same i. From the point of view of the program, the variable
would be named i, but from the assembly pov it would point to a different address.
The base vector address is shared, but each cache will be filled with different portions of the same array.

## MESI

Enhanced MSI, with an additional state that encodes exclusive and unmodified data. M state becomes "exclusive modified".

But that's the only form of incoherence.

Cache performance on symmetric SMMP is given by uniprocessor cache miss traffic, traffic caused by communication a the
fourth miss: coherence miss. (Compulsory, capacity, conflict, coherence).

Offset width determine cache granularity. A single cache line comprises all address with the same tag and index.
False sharing misses is specific to coherent caches, and generated by granularity.

Marking a cache as invalid is the same as erasing it. Blank line. If the processor attempts to read you have a coherency
miss.
Some sharing miss should not happen, but they do because of granularity.

In the slide 87 when P2 reads x2, we should have a hit however we have a false coherency miss since both variables belong
to the same cache line. This is the penalty we pay.

The main reason we create large blocks of cache is to exploit spacial locality.

> If using a location in memory, it's likely I'll use consequent locations shortly after.

A solution is spacing data allocation such that they're run in different cache lines.

## Alternatives

Cache coherency is also implemented in supercomputer-alike machines where there is distributed needs.

As said a CC system must provide a set of states, a state transition diagram and actions. To manage the coherence protocol
we need to determine when to invoke the protocol, then how to find the state of address in other caches, locate other
copies and ultimately invalidate/update copies.

The Snoopy protocol is based on broadcast: sending the same message to every actor. 1-to-all communication. This is also
the simplest way of implementing this. The downside is the cost of making the network. It's a viable option only if we
have a small set of cores and not in larger systems.

Let's consider a distribute memory scheme (each node has local memory, DRAM equivalent and an interconnect): even if a
NUMA node can access every other processor memory, it occurs a penalty. There's a little we can do for cache coherence
to scale.

cc-NUMA is quite more complex than the Snoopy protocol described so far. A solution is to partition cores into snooping
areas interconnected locally, then those areas are further connected on a second level of the snooping bus library.

The main bottleneck is related to performance and it doesn't really apply to any topology. In 3D sorted interconnections
doesn't really scale (designs are ultimately printed on silicon).

Cache coherence can also be achieved with directories: the assumption is that as more processors you have, only near
processors will share the same memory.

Directories are extensions to the cache that keep track of what processors have accessed which variables.
