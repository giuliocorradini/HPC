# The taxonomy of parallel architectures

What's the various types of parallelism available in modern computers?

|             | Single DS | Multiple DS |
|-------------|-----------|-------------|
| Single IS   | SISD      | SIMD        |
| Multiple IS | MISD      | MIMD        |

The classical CPU model is SISD, that means a single stream of instructions (implemented by the standard instructions
fetch logic), and a single stream of data going through the sole memory channel.

A very specific type of parallel architecture is MISD, where a single data pool is fed to multiple cores with multiple
instruction streams. We still have some example of MISD notions in array processors (or systolic array processors).
For example AMD Versal Core.

Some sort of SIMD logic was already included in the latest single-core CPUs (ever heard of SSE?). The computer exploits
multiple data streams against a single instruction stream for operations that can be naturally parallelized.

MIMD adds multiple data streams to SIMD. The most common architectures nowadays are SIMD and MIMD.

Programing styles arise from this taxonomy: SPMD has a single program that runs on all processors of an MIMD.
Each CPU points to the same code (we provide the same program counter), however their load store units will point
to different parts of the data. This is by far the most common programming style. In CUDA it's called STMD.

The easiest way to parallelize the program is to focus on loops, and compilers provide easy ways to parallelize loops.
This is normally achieved by making multiple threads that each carry out part fo the work.
Threads have visibility of the same memory space, and they are isolated (cannot communicate directly).

With threads all the requests map to the same virtual memory addresses. The drawback is a small overhead for orchestrating
the different CPUs. Overhead is the main reason of performance drop.

## SIMD

SIMD was basically specialized function units, so implemented in a co-processor.

Instruction level parallelism thrives to execute multiple instructions running concurrently, and data dependencies really
slow down this. Data level parallelism is simpler and as long as I'm not doing something that breaks data dependencies.

The simplest loop that can be parallelized is:

$$
for (int i=0; i<n; i++)
    a[i] = i;
$$

But when there are negative dependencies (the loop depends on the value computed at a previous step) I cannot execute
it in parallel. If I try to execute in parallel I break the semantic of the program.

DSP are coprocessor with massive SIMD architectures, because they're doing lots of maths. E.g. audio coprocessors
manipulate audio waves that are typically represented as long arrays. These are also called **vector units**.
Data-level parallelism was already a thing.

> In parallel programs, loop perform the same operation to a large number of items.

A super simple example is:

$$
for (i=0; i<1000; i++)
    x[i] = x[i] + s;
$$

Assuming each iteration takes N cycles to execute, the whole loop takes 1000 cycles. If we had logic that could do the
same operation N times and the same time, we could distribute it (remaining in the same CPU).

[x86 - Are SIMD and VLIW instructions the same thing? - Stack Overflow](https://stackoverflow.com/questions/70400206/are-simd-and-vliw-instructions-the-same-thing)

For the same program we have the SISD and SIMD execution in pseudocode:

SISD:

$$
for ...
$$

SIMD:

$$
for...
$$

### Automatic process

Parallelization with SIMD techniques is achieved by means of loop unrolling (and adjusting iteration rate accordingly).

If unrolling

$$
for (int i=0; i<1000; i++)
    x[i] = x[i] + s
$$

with the new stride it will become:

$$
for (int i=0; i<1000; i+=4)
    x[i] = x[i] + s
    x[i+1] = x[i+1] + s
    x[i+2] = x[i+2] + s
    x[i+3] = x[i+3] + s

$$

because the instructions of (1) are executed 4 times for each loop. Now we need to vectorize these operations.
Notice this is already a strenght reduced version of the loop, by removing the explict computation of i.

$$
L1:
    lf s0, 0(a0)
    add s0, s0, a1
    sd s0, 0(a0)
    addi a0, a0, 8
    bne a0, t0, L1
$$

By manually unrolling the loop:

$$
L1:
    ld s0, 0(a0)
    add s0, s0, a1
    sd s0, 0(a0)

    ld s1, 8(a0)
    add s1, s1, a1
    sd s1, 8(a0)

    ld s2, 16(a0)
    add s2, s2, a1
    sd s2, 16(a0)

    ld s3, 24(a0)
    add s3, s3, a1
    sd s3, 24(a0)

    addi a0, a0, 8
    bne a0, t0, L1
$$

I'm using different register names to not introduce artificial dependencies. I already reduced the overhead of loop control
operations.

Reorder because of continuosly spaced memory layout:

$$
    ld s0, 0(a0)
    ld s1, 8(a0)
    ld s2, 16(a0)
    ld s3, 24(a0)

    add s0, s0, a1
    add s1, s1, a1
    add s2, s2, a1
    add s3, s3, a1

    sd s0, 0(a0)
    sd s1, 8(a0)
    sd s2, 16(a0)
    sd s3, 24(a0)

    addi a0, a0, 8
    bne a0, t0, L1
$$

I can now load, manipulate and store data that is four times as large. These operations can ultimately be translated in
vectorized operations. This optimization can be generalized to any loop.

If the loop isn't an integer multiple of the k = SIMD width, then we will have a new unrolled loop that executes a
$floor({n \over k})$ times and a loop with the sequential execution of the body. Other approaches include array padding
to avoid the second loop.

## MIMD

Multicore architectures have at least 2 processors interconnected via a communication network. Where is the memory in
the system? Depends on whether the memory is shared or not.

SIMD executes in the same thread that exploits parallelism. In MIMD you also have to reason about workers: the software
entity that executes the programs. It's better to reason about threads, the primitive that implement workers and must
replicate.

MIMD have independent program counters, they can each run different functions. For n processors you need to have n threads
at least. The amount of computation that is assigned to each thread is called _granularity_.
Usually this type of parallelism is used only with small functions or loops (small granularity) since threads introduce
overhead.

Processors work in isolated environments, how do they communicate? We have two paradigms depending on the nature of the
application domain.

We can come up with two patterns:

- shared memory, the processors and memory are all interconnected by a network. It's a good representation of what you
have in your consumer electronics. One copy of data is shared among many cores but this introduces various problems.

- distributed memory on the other hand has dedicated memory block per processor. This means that when processors (or
threads) need to communicate have to specifiy which memory space they need. You now have several explict memory space
and the program must be aware of the location of data in this memory space.

The problem of caches in shared memory is completely orthogonal (as it's transparent to the programmer). The only place
data sits is the shared memory pool.

The distributed memory model is equivalent to an explicitly managed memory bus. A supercomputer is the simplest model
of distributed memory: each processor is a different node that has its memory and the interconnection network is Internet.

Let's say in a DM model a processor $P_i$ tries to access the memory of $P_j$, that of course cannot be done if not by
sending and receiving messages: data is explicitly copied across nodes.

In shared memory you have several threads that access the same data and access order becomes key, if not careful the developer
can break the semantics of the program. This is a problem about **synchronization** to rule out access of the same physical
addresses, and access the data in a proper order.

> Synchronization is the main problem in shared memory computation models

In distributed memory systems I have a messaging problem, involving data transfers, and writing code for these architectures
can be a very different story.

Beside that we have different parallelism types:

- data parallelism, involing computing the same operations on different data;

- task parallelism (or control parallelism), so perform different functions. This is the only type of parallelism that
can be done on non-parallel systems. The operating system has freedom to involve a new CPU to work on the program.
This is the type of parallelism typical of modern systems, that having different cores can run different programs truly
concurrently.

    The synchronization issues that may arise in the latter case (concurrent access to the camera or the GPU resources) can
be resolved by drivers or by the operating system that implements a policy of access.

In general we combine different types of parallelism at different level regardless of the memory model (shared or distributed).

## Shared memory programming

The simplest programming model.

All processors are, at some point of their lifetime, requiring a value $x$. You as a programmer don't have to worry about
the replicas of values and how to move them around, very similar to sequential programming done in C/C++.

At some points the main process can undock different threads to distribute the workload, this operation is called a fork,
and results is get by means of barriers with a join operation. This is the **fork-join model**.

Thread undocking is manually handled by the programmer, for example with the library _pthreads_.

[POSIX Threads Programming | LLNL HPC Tutorials](https://hpc-tutorials.llnl.gov/posix/)
