# OpenMP

GPU programming relies on vendor-specific languages such as CUDA.

OpenMP attempts to standardize, with a solution that provides a single interface for efficiently
programming the GPU and CPUs. Trying to limit the proliferation of programming models for CPUs and GPUs.

## A fast forward on GPGPUs

A modern GPU is typically organized in several clusters, called Streaming Multiprocessors in NVIDIA GPUs. Each SM has
a bunch of CUDA cores, and tensor cores which are novel accelerators for AI workloads.

The L1 can be partially configured as scratch memory and regular hardware managed cache.

## OpenMP heterogeneous

You have a single host device and one or more target devices. The coprocessors or GPUs are the target devices.

Memory is now explicitly divided in host memory and device memory. It mimics the fork-join model. The program begins as
usual by launching an initial thread running on the host device.

Note that there is an implicit parallel region that surrounds the entire program: execution is wrapped inside a task.

We have a new keyword `target`, for part of the program to be executed in a target device. Accelerator model is compliant
and is built on top of the tasking model.

We add two keyword for target, `map`. This semantic specifies where variables lies on which device.

```c++
double A[n, n], B[n, n], C[n, n];
#pragma omp target map(to: A, B) map(from: C)
```

A map clause entails some sort of data transfer. These will more or less impact the execution.

When marking function definitions or declaration you can use `omp declare target` pragma.

## Runtime functions

We have some runtime functions used to manage the accelerator. In this case you will have several devices and you have
functions to discriminate against the tasks.

If you don't specify the device you upload to, the OMP_DEFAULT_DEVICE environment variable will contain the ID of the
default target device for acceleration.

It's important that your application has awareness of the GPU cluster architecture.

## Offloading

To achieve offloading, which is desirable, you need to add the `nowait`. At some point in your program you will have
dependencies, so either do some other work and read the results, or add a synchronization directive to wait for the
offloaded work to complete.

## Teams

The `teams` constructs creates a league of thread teams. The master thread executes the teams region. Threads are synchronized
locally in a cluster and cannot sync across clusters. The reason is ultimately architectural.

OpenMP was crafted after the features of modern GPUs. The hardware vendor are pushing for a programming language that
matches their features. It's easy to sync in a cluster. GPU don't want to sync across clusters, as it kills performance and
is not supported in hardware.

When using target, copy-based semantics is assumed. Inside of the device you can use shared again.

## SAXPY on device

The computation is a scalar a x plus y, and it's a pattern. The name comes from this scalar multiplication.

> Very representative type of kernel, which is the core of computation.

You can distribute work in a cluster-aware manner using teams.

You can go for more fine-grained SIMD parallelism inside each thread using `pragma omp parallel for simd`.

Say you have 64 iterations in you rloop, assigned to 2 teams. Each team has four threads and each thread has 2 SIMD
lanes. It's all hierarchic, and you have to consider the characteristics of the device by mapping the constructs tied
to an architecture.

Distribution patterns can be inconvenient.

Functions in a target region will be compiled for both host and target device.
